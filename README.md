# Challenge 1B â€” Offline PDF Mining, Ranking & Summarization (CPU)

> Extract **structured sections** from collections of PDFs and output a compact JSON with (1) **Topâ€‘N sections** and (2) **short refined snippets** most relevant to a **persona** and **jobâ€‘toâ€‘beâ€‘done**.
> Runs **fully offline** inside Docker on **CPU**, with strict thread caps for predictable speed.

---

## ðŸš€ Dockerâ€‘First Quickstart

### 1) Build the image

```bash
# From the repo root (where the Dockerfile and code live)
docker build -t pdf-processor-1b:latest .
```

### 2) Place inputs & (optional) local models

```
.
â”œâ”€ Collection 1/
â”‚  â”œâ”€ challenge1b_input.json
â”‚  â””â”€ PDFs/
â”‚     â”œâ”€ file1.pdf
â”‚     â””â”€ file2.pdf
â”œâ”€ Collection 2/
â”‚  â”œâ”€ challenge1b_input.json
â”‚  â””â”€ PDFs/...
â””â”€ models/
   â”œâ”€ publaynet_d0_min.pth.tar       # (optional) PubLayNet EfficientDetâ€‘D0
   â”œâ”€ msmarco-miniLM/                # (optional) CrossEncoder folder
   â””â”€ all-MiniLM-L6-v2/              # (optional) SentenceTransformer
```

### 3) Run (Windows PowerShell)

```powershell
docker run --rm `
  -v "${PWD}:/app" `
  --env WORKERS=8 `
  --env SKIP_LP_INIT=0 `
  --env USE_CE_RERANK=1 `
  --env EXCLUDE_CAP=60 `
  --network none `
  pdf-processor-1b:latest
```

### 3â€™) Run (Linux/macOS bash)

```bash
docker run --rm \
  -v "$PWD:/app" \
  -e WORKERS=8 \
  -e SKIP_LP_INIT=0 \
  -e USE_CE_RERANK=1 \
  -e EXCLUDE_CAP=60 \
  --network none \
  pdf-processor-1b:latest
```

**Outputs** are written **in place** under each collection folder:

```
Collection X/challenge1b_output.json
```

---

## âœ¨ What You Get

* **Parallel PDF extraction** with `ProcessPoolExecutor`. Use `WORKERS=<cores>`.
* **Robust section detection**

  * **PubLayNet** (EfficientDetâ€‘D0 via LayoutParser) if `models/publaynet_d0_min.pth.tar` exists and `SKIP_LP_INIT=0`.
  * **Heuristic fallback** (no weights needed): titles by **font size + UPPERCASE ratio + length**, body by line accumulation.
* **Dataâ€‘driven constraints (offline)**

  * **YAKE** â†’ **include** keywords for the persona+task.
  * **WordNet** expansions (synonyms/hyponyms/antonyms) â†’ **includes/excludes/hints**.
  * **`EXCLUDE_CAP`** prevents huge exclude lists from tanking speed/quality.
* **Hybrid ranking**

  * TFâ€‘IDF + page/length priors + lexical coverage + **constraint gating**.
  * Optional **CrossEncoder** reâ€‘rank when `USE_CE_RERANK=1` and local CE model exists.
  * **MMR** for diverse Topâ€‘K (bugâ€‘fixed to avoid rare `KeyError`).
* **Refined snippets**

  * Best short paragraph (\~â‰¤800 chars) per selected section, ending on sentence boundaries.
* **Strict thread caps**

  * Common BLAS/OpenMP threads pinned to 1 to avoid oversubscription.

---

## ðŸ§¾ Input JSON (per collection)

`Collection X/challenge1b_input.json`

```json
{
  "persona": { "role": "Travel Planner" },
  "job_to_be_done": { "task": "Plan a trip of 4 days for a group of 10 college friends." },
  "documents": [
    { "filename": "South of France - Cities.pdf" },
    { "filename": "South of France - Beaches.pdf" }
  ]
}
```

---

## âš™ï¸ Environment Variables

| Variable                 | Default | Purpose                                                                            |
| ------------------------ | ------: | ---------------------------------------------------------------------------------- |
| `WORKERS`                |     `1` | Number of **processes**. Set to **physical cores** (e.g., 8).                      |
| `SKIP_LP_INIT`           |     `0` | `0` = try PubLayNet if weights exist; `1` = **heuristic only**.                    |
| `USE_CE_RERANK`          |     `0` | `1` = use local CrossEncoder to reâ€‘rank topâ€‘M candidates (if model folder exists). |
| `EXCLUDE_CAP`            |    `60` | Hard cap for expanded **excludes** from WordNet.                                   |
| `TOP_K`                  |     `5` | Final number of results (constant in code unless changed).                         |
| `RERANK_TOP_M`           |    `40` | Candidate count for CE reâ€‘rank before blending.                                    |
| `CE_BLEND`               |  `0.45` | Blend weight for CE score into final score.                                        |
| `EMBED_BATCH`            |     `8` | Batch size for SentenceTransformer embeddings.                                     |
| `MAX_SECTIONS_FOR_EMBED` |   `180` | Preâ€‘selection cap before heavier scoring.                                          |

> The container also sets common CPU thread envs (OMP, MKL, etc.) to **1** to keep each worker singleâ€‘threaded for predictable speedups.

---

## ðŸ“¦ Optional Local Models (offline)

Place **inside your mounted repo** (so the container sees them at `/app/models/...`):

* **PubLayNet EfficientDetâ€‘D0 weights**
  `models/publaynet_d0_min.pth.tar`
* **CrossEncoder** (e.g., MS MARCO MiniLM)
  `models/msmarco-miniLM/` with `config.json`, `pytorch_model.bin`, `tokenizer.*`
* **SentenceTransformer** (e.g., `all-MiniLM-L6-v2`)
  `models/all-MiniLM-L6-v2/` with model + tokenizer files

> The container runs **offline** (`--network none`), so ensure these folders are complete if you enable those features.

---

## ðŸ§­ How It Works (Short)

1. **Discover collections**: scan subfolders for `challenge1b_input.json`.
2. **Extract sections in parallel**: PubLayNet (if available) or heuristic fallback. Normalize text; attach `{source_pdf, page_number}`.
3. **Build constraints** from persona+task: YAKE + WordNet â†’ includes/excludes/hints (capped).
4. **Preâ€‘select** (optional): TFâ€‘IDF shortlist if many sections.
5. **Score & gate**: TFâ€‘IDF + priors + lexical coverage + include/exclude/hints gates.
6. **Reâ€‘rank** (optional): local CE reâ€‘rank on topâ€‘M and blend score.
7. **Select Topâ€‘K**: MMR for diversity (fixed argmaxâ€‘inâ€‘candidateâ€‘set).
8. **Refine snippets**: choose best paragraph per selection (\~â‰¤800 chars).
9. **Write** `challenge1b_output.json` inside each collection.

---

## ðŸ§ª Example Logs

```
--- Running with WORKERS=8 (mode: LP+EffDet, CE_rerank=yes) ---
Processing Collection 1 â€¦
[parallel] Extracting 15 PDFs with 8 workers â€¦
  â†³ 2/15 PDFs done
  â†³ 4/15 PDFs done
  ...
[parallel] Done in 36.5s â€“ 192 sections.
[rank] 192 sections â†’ selectingâ€¦
[rank] includes=10 excludes=60 texts=11 (ST=no, CE_rerank=yes)
[write] Collection 1/challenge1b_output.json
```

---

## ðŸ§¾ Output Schema

```json
{
  "metadata": {
    "mode": "LP+EffDet | heuristic",
    "workers": 8,
    "timestamp": "2025-07-28T18:40:13Z"
  },
  "extracted_sections": [
    {
      "document": "South of France - Cities.pdf",
      "section_title": "Top Cities for Group Trips",
      "importance_rank": 1,
      "page_number": 3
    }
  ],
  "subsection_analysis": [
    {
      "document": "South of France - Cities.pdf",
      "refined_text": "Nice offers connected beach promenades, groupâ€‘friendly hostels...",
      "page_number": 3
    }
  ]
}
```

---

## âš¡ Performance Tips

* Set `WORKERS =` **physical cores** (e.g., 8).
* PubLayNet improves **title quality** but adds compute; heuristic mode is fastest.
* Very high DPI or very large PDFs increase runtimeâ€”tune if needed.

---

## ðŸ›  Troubleshooting

* **MMR `KeyError`**: fixed (argmax taken over current candidate set each iteration).
* **No CE reâ€‘rank** despite `USE_CE_RERANK=1`: check `models/msmarco-miniLM/` exists with model + tokenizer.
* **PubLayNet not activating**: ensure `SKIP_LP_INIT=0` and `models/publaynet_d0_min.pth.tar` exists.
* **Weird bullets or ligatures**: normalization handles most; extend bullet list if your PDFs are exotic.
* **Slow even with many workers**: confirm thread caps are 1 (container sets them), reduce DPI, or use heuristic mode (`SKIP_LP_INIT=1`).

---

## ðŸ“œ License & Attribution

* PubLayNet, LayoutParser, YAKE, NLTK/WordNet, and any model artifacts follow their respective licenses.
* Repository code is MIT (unless noted otherwise).

---

## ðŸ™Œ Acknowledgements

Thanks to the PubLayNet & LayoutParser communities, and the YAKE/NLTK teams for enabling practical offline NLP/CV.

---

### Oneâ€‘liner (PowerShell)

```powershell
docker run --rm `
  -v "${PWD}:/app" `
  --env WORKERS=8 `
  --env SKIP_LP_INIT=0 `
  --env USE_CE_RERANK=1 `
  --env EXCLUDE_CAP=60 `
  --network none `
  pdf-processor-1b:latest
```

### Oneâ€‘liner (bash)

```bash
docker run --rm -v "$PWD:/app" -e WORKERS=8 -e SKIP_LP_INIT=0 -e USE_CE_RERANK=1 -e EXCLUDE_CAP=60 --network none pdf-processor-1b:latest
```
